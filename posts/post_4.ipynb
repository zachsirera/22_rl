{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22_rl: Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.game import Game\n",
    "from classes.player import Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this problem, the entire basis for intelligent decision making is a matrix that an agent uses to make an informed decision. This decision matrix outlines all of the state-action pairs an agent may face and the value of that decision; a measure of how likely it is to lead to a positive or negative outcome. An agent playing the game simply evaluates its decision matrix for all of its possible actions at its current state and picks the action with the greatest value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of the learning process the agent initializes its decision matrix with uniform 0 values, representing an agent who knows nothing about the problem environment. When an agent plays the game and it encounters multiple equivalent values for each action, it will choose between them at random to explore the state space. An agent that doesn't learn will pick its actions completely at random at each decision point. To demonstrate this, let's run 100 games between two, three, and four unintelligent agents and determine the win percentage of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 players win percentage:  [0.54, 0.48]\n",
      "3 players win percentage:  [0.36, 0.34, 0.3]\n",
      "4 players win percentage:  [0.25, 0.27, 0.28, 0.23]\n"
     ]
    }
   ],
   "source": [
    "n_players = [2, 3, 4]\n",
    "n_cards = 7\n",
    "\n",
    "n_players = [2, 3, 4]\n",
    "\n",
    "for n in n_players:\n",
    "    players = [Player(j, random_play=True) for j in range(n)] \n",
    "\n",
    "    for i in range(100): # play 100 games\n",
    "        game = Game(players, n_cards)\n",
    "        game.play()\n",
    "\n",
    "    print(str(n) + \" players win percentage: \", [round(x.losses / x.games_played, 4) for x in players])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, three agents all choosing randomly achieve similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q Learning, agents iteratively evaluate the following Bellman equation to determine the Q value of each state.\n",
    "\n",
    "![Q Learning](../assets/q_algo.png \"Q Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind this equation is pretty simple. Every time an agent visits a state it reevaluates the states Q value by incrementing it with a factor representing the \"temporal difference\". Simply put, this temporal difference represents the reward or penalty of the game, discounted by how far removed it is from the terminal state-action of a game. In this problem, there will be a single penalty for losing and there will be n - 1 rewards, where n is the number of players, representing the shared value of winning. If a loss value is -10 and there are four players, the win value will be 10/3, for example. The reason for this is that the goal of 22 is not necessarily to win, it is to not lose. Under this reward scheme the agent will prioritize states that avoid losing over seeking out states that encourage winning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trickiest part of this equation is the max Q term. In 22 an agent will never play inside the same context twice. What I mean by this is that an agent only gets to make a play once in each round. Because of this there is a disconnect between the current state and the set of possible next states which makes it difficult to evaluate this term. The way I have chosen to handle this is to have an agent consider its following possible states as either leading, or all of the possible 1 and 2-card plays it might face. The reason for excluding 3-card plays in this is that these are decently rare plays and because of this a human playing the game probably won't look at their hand at every play and ask what they would do if an opponent led a three of a kind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially learning will be based on winning versus losing, but subsequent learners will learn on a round-by-round basis based on how they accumulate points. At the end of each game, the agent will revisit the decisions it made to get to the outcome and will evaluate the Bellman equation and determine the new Q value for each state with the following method of the Player class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(self, states, outcome, n_players, learning_rate=0.1, discount_factor=0.95): \n",
    "    '''\n",
    "    function to apply the Q Learning algorithm to the problem. \n",
    "    information can be found here: https://en.wikipedia.org/wiki/Q-learning \n",
    "    '''\n",
    "\n",
    "    # traverse states in reverse to apply the discount factor\n",
    "    for i, state in enumerate(states[::-1]):\n",
    "        q_old = self.decision_matrix[state[0]][state[1]]\n",
    "\n",
    "        if i == 0:\n",
    "            if outcome: # outcome == True represents a win, False a loss.\n",
    "                reward = 10 / (n_players - 1) # reward for winning is shared among players\n",
    "            else:\n",
    "                reward = -10 # penalty for losing is exclusive to the loser\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        q_max = -10\n",
    "\n",
    "        for row in self.decision_matrix[:182]:\n",
    "            # get the lead, 1 card plays, and 2 card plays. 1 + 13 + 169 rows. \n",
    "            for val in row[:182]:\n",
    "                if val > q_max:\n",
    "                    q_max = val \n",
    "\n",
    "        # update decision matrix with new Q value\n",
    "        self.decision_matrix[state[0]][state[1]] = q_old + learning_rate * (reward + discount_factor * q_max - q_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Post 5: Initial Experimentation](https://github.com/zachsirera/22_rl/blob/main/posts/post_5.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
