# 22_rl: *Attempting to* Apply Reinforcement Learning principles to the card game 22. 

## Background

The rules of the game 22 are simple but I find it easiest to explain in reverse. Every player starts with a score of 0. Each game is composed of a number of rounds, and at the end of each round every player is left holding one card. The player holding the card of the highest value keeps that card and adds it to their "score". The game ends when one person has accumulated a score greater than or equal to 22. All face cards are worth ten points and an Ace is worth eleven. 

Each round consists of multiple hands and each hand starts with a player "leading" or starting the hand. The leader gets to dictate the structure of the hand by electing to play a single card, or multiple cards of the same value, if any such combinations are present in their hand. Following this, each player in the game must respond in order by playing the equivalent number of cards that either beat or tie the current play. An important clarification is that if the leader plays two eights, for example, the next player does not also have to play two of a kind to tie or beat the play. They can play two eights, an eight and a nine, a nine and an ace, or two aces. If a player does not have a card or cards in their hand that can beat or tie, they must "burn" the lowest card or cards in their hand. A player can also strategically "play under" by burning their lowest card or cards if they so choose, preserving higher value cards for later in the round. A player does not have to inform the rest of the players whether they are playing under or simply cannot beat the current play. The hand is complete when all players have responded to the initial lead and the last player able to tie or beat the current play gets to lead the next hand. They can then start the process over and lead whatever card or matching cards that they want, provided that they leave at least one card left in their hand and everyone else's hand who is playing the game. When the round finishes one player must keep their card and the deck is reshuffled and the next round is started. 

I was introduced to the game 22 in 2014 when I was working as an intern at a chemical plant, a game all of the other engineers played at lunch. I started playing the game much like many people do, without a clear grasp of what a non-losing strategy looks like and with a tolerance for risk that is more appropriate for games where you are trying to win, not to not lose. Playing to "win", or have the lowest card at the end of the round, can be entertaining, but the game doesn't care if you win by a single point or if you win by ten points. Many new players to 22 end up making risky plays or improperly valuing certain cards in their hand which ultimately causes them to lose, often times spectacularly, at the beginning. However, this delayed feedback allows players to develop strategies that hopefully improve their odds over time. An individual's strategy is shaped slowly over time, in exactly the same way as a reinforcement learner, by trying to determine what moves led to a player losing a hand, a round, and eventually a game and adjusting future play accordingly. 

22 is an interesting game from an RL standpoint because of this insistance on not losing rather than winning. It is a card game after all, so there is no perfect strategy. Probability theory dictates that every player employing a similar strategy will lose 1/n of the time, where n is the number of players in the game. But there exists somewhere some optimal strategy that consists of taking the optimal action at each state in the game and simply relying on one of your opponents to make an incorrect decision or to be unlucky. This means taking the same action in each revisited state because this action gives an individual the greatest chance of success. Because of this "algorithmic" application of an individual strategy, I believe that the game 22 is well-suited to the application of Reinforcement Learning.

It didn't take long after I started playing 22 to realize that I wanted to approach the game from some computational angle. I had some vague notion of Game Theory concepts like minimax and maximin that I knew I was applying while I played the game, but I didn't understand the formalism behind them. Furthermore, I didn't have the analytical skills to tackle a project like this. Until now. 

This project represents my attempt to apply Reinforcement Learning, specifically Model-Free Reinforcement Learning, to the card game 22 to attempt to elucidate an optimal strategy for all situations in the game. 

