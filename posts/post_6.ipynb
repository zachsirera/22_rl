{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22_rl: In-depth Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents in this module apply an \"epsilon-greedy\" exploration stategy. Epsilon exploration boils down to establishing a probabilty threshold, epsilon, and then using this probability to select an option other than the optimal choice. In this case the optimal choice is the max value from the Q matrix for all successive states. \n",
    "\n",
    "Here is an example of what an epsilon-greedy algorithm might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random.random() >= self.epsilon:\n",
    "    play = max(vals, key=lambda x:x ['value'])\n",
    "else:\n",
    "    play = random.choice(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What an epsilon-greedy exploration strategy allows the agent to do is to avoid only picking the optimal value at any given decision point, encouraging the agent to explore the problem space. You can imagine that when an agent encounters a situation early in its learning process that it determines to lead to winning it will just pick that same action every time it arrives in the same state unless it loses enough to weight it below the other actions in that state. This is obviously problematic because it allows the agent to home in on solutions that may be sub-optimal just because it encountered them early; it closes the feedback loop between decisions and outcome before the agent has the chance to understand all of its actions. \n",
    "\n",
    "Using an epsilon-greedy algorithm avoids this, because it allows the agent to pick options that might not be \"optimal\" at every step in an effort to understand the problem space. However, at some point we expect the agent to understand the landscape of the game and thus to only pick the optimal action. To achieve this, we can decay epsilon as the agent makes more decision. So initialy the agent can start with a relatively high epsilon value, picking almost randomly at each decision point, but over time this value decreases, forcing the agent to pick the \"greedy\" option at all decision points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Self-play Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first round of experiments involves having agents play games againts themselves and then stopping periodically to evaluate how a \"learned\" agent performs against agents who play totally randomly. It is possible to play two, three, or four-person games of 22, and each comes with its own strategy/risk tolerance, so for this step I trained and tested agents in two, three, and four-person configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the visualizations of the Q-matrix after 1,000,000 games of self-play in each number of players, 2, 3, and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![One Card Decision Matrices](../figures/combined_dm_1c_1000000.png \"One Card Decision Matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one-card hands, some interesting strategy emerges. Agents seem to learn, regardless of the number of players, that it is a bad idea to play their lower-value cards in reseponse to lower value cards. Interestingly, the three and four-player learned agents learn that \"playing under\", or sacrificing their lower value cards strategically rather than beating the current play, is more valuable in most situations. I would not expect such a consensus. \n",
    "\n",
    "For the two-player matrix, the agent is able to distinguish between \"good\" plays and \"bad\" plays a little bit more clearly. This is evidenced by the average \"color\" of the matrix being a greenish shade, halfway between the negative purple hue and the positive yellow hue. We would expect that most plays are not such clear predictors for success and failure and that most plays are only mildly correlated. \n",
    "\n",
    "In the three-player matrix we see sort of the opposite. The agent assigns almost all plays a discounted positive value. This is counterintuitive, because we shouldn't necessarily assume that all plays are likely to lead to a positive outcome, unless dictated as such by the reward structure. In this experiment, the agents are playing a zero-sum game. There is a singular, fixed cost for losing, but there is a shared reward for winning. An agent who loses gets penalized -X points, but an agent who wins gets rewarded +X/n points, where n represents the number of other winners in the game instance. In a two-player game this zero-sum nature allows for the agent to learn strategies to win at an equal weight as it is trying to learn strategies to avoid losing. In higher-order games, the agent simply needs to learn to avoid losing, so it is less focused on picking the best option, and is satisfied for options that allow its opponent to lose based on probability. A different reward structure might change this sort of behavior. \n",
    "\n",
    "In the four-player matrix we start to see some disturbances. No clear strategy emerges. I believe this has to do with the conflicting nature of the game and Q Learning. The agent makes its decision by trying to pick the action that will maximize the discounted reward at its next state. This is relatively easy to do in a two-player game, and slightly easier to do in a three-player game, but becomes much more challenging in a four-player game. In a two-player game, the agent can be reasonably sure what is going to happen after each action. As you add more players to the game, this is not as easy to do. This induces some error in the Q Matrix as the agent becomes less and less certain what moves it will see next. However, I am not certain that this variability in score is a bad thing. Like I said above, I would expect that many of the moves in the matrix don't correlate as strongly with positive or negative outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two Card Decision Matrices](../figures/combined_dm_2c_1000000.png \"Two Card Decision Matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two-card hands we start to see some structure emerge in the strategy/probability distribution of the game. \n",
    "\n",
    "In two-player games the only two-card hands that an agent will face are pairs, because that is the only legal lead that a player can play. So we see a series of lines representing all of the options to play on top of each face-face pair in the game. For three and four players, we start to see some ranges. A player might lead a pair of sixes, and the second player in the game can play whatever two legal cards they want on top of those cards. The third player, then, is not guaranteed to play on top of a pair so they must be prepared for a range of possible cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three Card Decision Matrices](../figures/combined_dm_3c_1000000.png \"Three Card Decision Matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately for three-card plays, the agents did not update their decision matrices. I'll have to track this down to understand exactly why this is happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the visualizations of the decision matrices are interesting, but the real question is how well did the agents perform?\n",
    "\n",
    "The key metric for the agents is their \"success rate\", which I am defining as the portion of games a learned agent wins against a range of opponents divided by the win rate expectation assuming that the opponents and the agent are all equally likely to win. In other words, it is the win rate divided by a number representing how likely the agent is  to win. For two players, this denomenator is 0.5 because two equally-skilled agents should each expect to win about half the time. For three this number is 0.67 and for four this number is 0.75. So a success rate of 1 implies that an agent does no better than its opponents on average. Greater than 1 implies that the agent is better than its opponents and wins more than its fair share, and less than 1 implies the agent is worse. \n",
    "\n",
    "The testing structure is as follows, the game is initialized with n players and these players play 10,000 games. Then one of the agents is taken out of the testing setup and plays 100 games against n - 1 players who haven't learned how to play the game and its success rate is recorded. This is repeated 100 times so that the learned agents have played 1,000,000 games total. The results are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two Player Learning Curve](../figures/lc_2p_1000000.png \"Two Player Learning Curve\")\n",
    "![Three Player Learning Curve](../figures/lc_3p_1000000.png \"Three Player Learning Curve\")\n",
    "![Four Player Learning Curve](../figures/lc_4p_1000000.png \"Four Player Learning Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the two-player agent is able to perform significantly better than its unintelligent opponent, with an average success rate of 1.2 over the duration of the experiment. The three-player agent performs only slightly better, with an average success rate of 1.02. And the four-player agent performs significantly worse than its opponent, with an average success rate of only 0.92. \n",
    "\n",
    "As I briefly explained above, I believe this has to do with the nature of the game and how it conflicts with the paradigm of Q learning. In a two-player game, the agent can always know whether the play it makes will win a hand and secure the lead going into the next hand or if it will concede the lead to its opponent. The agent doesn't always know  the next state that it will find itself in, but it can predict a little more accurately. This becomes significantly less likely in four-player games, because of the increased degrees of freedom in the system. There are many possible hands out there, it is difficult to predict if the opponents who follow will beat an agent's play or not. \n",
    "\n",
    "Some interesting behavior emerges in the plots though. For all agents we see some periods of increased success rates initially where the agent appears to learn some information about the game that allows it to improve its performance. Obviously there is some luck involved and no strategy can beat a player with perfect cards, but we see multiple consecutive increases insuccess rate as the agent plays more games. However, at some point we see this start to turn and the agent starts to lose more frequently. Its opponents aren't getting better, they don't learn. They pick totally randomly. So what is happening? I believe what is happening in these regions is that the agent is overfitting success in regions of the game that don't always translate to other regions. In other words, the agent is getting too confident in its strategy and eventually it is forced to correct this based on the probability distribution of hands in a game. This process happens repeatedly, but it appears to converge towards the average as the agent plays more games. In other words, the agent refines a strategy that more accurately reflects the nature of the game. \n",
    "\n",
    "There are a couple of things I want to experiment with next, but have to decide the best way to proceed in the interest of time. These 3,000,000+ games took my computer almost two weeks to run. To be able to refine and distill a clear, optimal strategy, I would like to be able to conduct experiments more rapidly. The first option that I plan to pursue is to let the agent learn over rounds instead of just games, and build a reward structure that reflects this. In other words, the agent will really want to avoid losing, but it will want to avoid accumulating points at all. This seems intuitive to humans as they play the game, because more points means a greater chance of losing at any given time, but the agent at the moment has no notion of what accumulating points means for its strategy beyond just understnading that the moment it accumulates points greater than or equal to 22 it has lost. I believe I can bake this into the reward structure rather than make any structural changes to the decision matrix, which would increase the complexity of the surface of the game. \n",
    "\n",
    "Until next time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
