{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22_rl: In-depth Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents in this module apply an \"epsilon-greedy\" exploration stategy. Epsilon exploration boils down to establishing a probabilty threshold, epsilon, and then using this probability to select an option other than the optimal choice. In this case the optimal choice is the max value from the Q matrix for all successive states. \n",
    "\n",
    "Here is an example of what an epsilon-greedy algorithm might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random.random() >= self.epsilon:\n",
    "    play = max(vals, key=lambda x:x ['value'])\n",
    "else:\n",
    "    play = random.choice(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What an epsilon-greedy exploration strategy allows the agent to do is to avoid only picking the optimal value at any given decision point, encouraging the agent to explore the problem space. You can imagine that when an agent encounters a situation early in its learning process that it determines to lead to winning it will just pick that same action every time it arrives in the same state unless it loses enough to weight it below the other actions in that state. This is obviously problematic because it allows the agent to home in on solutions that may be sub-optimal just because it encountered them early; it closes the feedback loop between decisions and outcome before the agent has the chance to understand all of its actions. \n",
    "\n",
    "Using an epsilon-greedy algorithm avoids this, because it allows the agent to pick options that might not be \"optimal\" at every step in an effort to understand the problem space. However, at some point we expect the agent to understand the landscape of the game and thus to only pick the optimal action. To achieve this, we can decay epsilon as the agent makes more decision. So initialy the agent can start with a relatively high epsilon value, picking almost randomly at each decision point, but over time this value decreases, forcing the agent to pick the \"greedy\" option at all decision points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Self-play Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first round of experiments involves having agents play games againts themselves and then stopping periodically to evaluate how a \"learned\" agent performs against agents who play totally randomly. It is possible to play two, three, or four-person games of 22, and each comes with its own strategy/risk tolerance, so for this step I trained and tested agents in two, three, and four-person configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
