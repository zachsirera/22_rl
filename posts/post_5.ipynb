{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22_rl: Initial Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial experimentation is done with two players and with little exploration. What this means is that the agent is going to evaluate its actions at each state and it is always going to choose the action that maximizes its expected Q Value. More sophisticated agents will choose with some degree of randomness if they are not confident there is a \"best\" play available to them. This is to encourage the agents to explore the surface of the problem space more and should hopefully lead to better performance over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10,000 games: 2-player, no exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment is just to let two agents play 10,000 games and learn some strategy along the way. Then take one of these trained agents and have them play 100 games against an agent who plays totally randomly and observe the perforamnce. 10,000 is a relatively small number of games compared to the entire state space of the problem, so I don't expect performance to be spectacular. But I want to visualize the Q matrix for the agent after 10,000 games to see how learning is progressing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single Card Plays](../figures/single_card_10000.png \"Single Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision matrix for single card plays we can see some interesting patterns that are confidence-inspiring. The most notable is the upper-left corner representing an agent playing its low card in response to the opponent's low card. The agent is able to discern this is a bad move, low value represented by the dark color, because the result is counterproductive in the long run as it removes valuable low cards from its hand. The slightly lower value diagonal is interesting as well. The agent would rather not play a card equivalent in value to the card being played, but this might be a result of the relative rarity of having a single card which matches the single card in the play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two Card Plays](../figures/two_card_10000.png \"Two Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three Card Plays](../figures/three_card_10000.png \"Three Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the agent faces relatively few 2-card plays and what appears to be no 3-card plays, but it is not clear if there are no values for this matrix or if they are just in pockets small enough to not appear, since this graph has 2197 units on each axis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:  2  players\n",
    "\n",
    "2 players win percentage:  [0.5321, 0.4734]\n",
    "\n",
    "Trained Agent aganst a random player\n",
    "\n",
    "2 players win percentage:  [0.62, 0.38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the results from the first 10,000 game epoch. 10,000 games is far too few games to fully explore the game's surface, but we still see some improvement when the agent plays a random player, winning almost two thirds of the games they play. However, I don't think there is much to take away from this. I think this has more to do with luck than anything. 100 games is still relatively few to smooth out some of the effects of random chance that come with the game, especially considering that the \"learned\" agent is still playing mostly random as it hasn't had enough opportunity to visit the most likely regions in the game's surface enough and little opportunity at all to visit the less likely regions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Post 6: In-depth Experimentation](https://github.com/zachsirera/22_rl/blob/main/posts/post_6.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
