{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22_rl: Initial Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial experimentation is done with two players and with little exploration. What this means is that the agent is going to evaluate its actions at each state and it is always going to choose the action that maximizes its expected Q Value. More sophisticated agents will choose with some degree of randomness if they are not confident there is a \"best\" play available to them. This is to encourage the agents to explore the surface of the problem space more and should hopefully lead to better performance over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10,000 games: 2-player, no exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment is just to let two agents play 10,000 games and learn some strategy along the way. Then take one of these trained agents and have them play 100 games against an agent who plays totally randomly and observe the perforamnce. 10,000 is a relatively small number of games compared to the entire state space of the problem, so I don't expect performance to be spectacular. But I want to visualize the Q matrix for the agent after 10,000 games to see how learning is progressing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single Card Plays](../figures/single_card_10000.png \"Single Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision matrix for single card plays we can see some interesting patterns that are confidence-inspiring. \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two Card Plays](../figures/two_card_10000.png \"Two Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three Card Plays](../figures/three_card_10000.png \"Three Card Plays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:  2  players\n",
    "\n",
    "2 players win percentage:  [0.5321, 0.4734]\n",
    "\n",
    "Trained Agent aganst a random player\n",
    "\n",
    "2 players win percentage:  [0.4, 0.62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
